{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pattern_classify.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Coreight98/Paran_ajou_Project_Anything/blob/main/alexnet_test1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgzZztiFkeEI",
        "outputId": "bbd1c679-f0e2-4b6c-8d5c-5b75ceceefb1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbxdrJHMLq8N",
        "outputId": "30a48f7e-691c-4985-8163-8db2fec45501",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 21.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (54.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOAJo9kUvZTK"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-l00FsbwGkf"
      },
      "source": [
        "# define model parameters\n",
        "NUM_EPOCHS = 10  # original paper\n",
        "BATCH_SIZE = 4\n",
        "MOMENTUM = 0.9\n",
        "LR_DECAY = 0.0005\n",
        "LR_INIT = 0.01\n",
        "IMAGE_DIM = 227  # pixels\n",
        "NUM_CLASSES = 38  # 38 classes \n",
        "DEVICE_IDS = [0, 1, 2, 3]  # 멀티 GPU\n",
        "\n",
        "# modify this to point to your data directory\n",
        "#test_location = './drive/MyDrive/data/DTD_test/'\n",
        "TRAIN_IMG_DIR = './drive/MyDrive/data/Alexnet_test/DTD/'\n",
        "OUTPUT_DIR = './drive/MyDrive/data/Alexnet_test/Output/'\n",
        "LOG_DIR = OUTPUT_DIR + '/tblogs'  # tensorboard logs\n",
        "CHECKPOINT_DIR = OUTPUT_DIR + '/DTD_models_cp'  # model checkpoints\n",
        "\n",
        "# make checkpoint path directory\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJOnVvPBpXqZ"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model consisting of layers propsed by AlexNet paper.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=38):\n",
        "        \"\"\"\n",
        "        Define and allocate layers for this neural net.\n",
        "        Args:\n",
        "            num_classes (int): number of classes to predict with this model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # input size should be : (b x 3 x 227 x 227)\n",
        "        # The image in the original paper states that width and height are 224 pixels, but\n",
        "        # the dimensions after first convolution layer do not lead to 55 x 55.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),  # (b x 96 x 55 x 55)\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),  # section 3.3\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 96 x 27 x 27)\n",
        "            nn.Conv2d(96, 256, 5, padding=2),  # (b x 256 x 27 x 27)\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 13 x 13)\n",
        "            nn.Conv2d(256, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 384, 3, padding=1),  # (b x 384 x 13 x 13)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(384, 256, 3, padding=1),  # (b x 256 x 13 x 13)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),  # (b x 256 x 6 x 6)\n",
        "        )\n",
        "        # classifier is just a name for linear layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        self.init_bias()  # initialize bias\n",
        "\n",
        "    def init_bias(self):\n",
        "        for layer in self.net:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "        # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\n",
        "        nn.init.constant_(self.net[4].bias, 1)\n",
        "        nn.init.constant_(self.net[10].bias, 1)\n",
        "        nn.init.constant_(self.net[12].bias, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Pass the input through the net.\n",
        "        Args:\n",
        "            x (Tensor): input tensor\n",
        "        Returns:\n",
        "            output (Tensor): output tensor\n",
        "        \"\"\"\n",
        "        x = self.net(x)\n",
        "        x = x.view(-1, 256 * 6 * 6)  # reduce the dimensions for linear layer input\n",
        "        return self.classifier(x)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvP3CWQiBXdJ",
        "outputId": "0ac8531b-a997-4fab-d6f0-6629bdf5773e"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # print the seed value\n",
        "    seed = torch.initial_seed()\n",
        "    print('Used seed : {}'.format(seed))\n",
        "\n",
        "    tbwriter = SummaryWriter(log_dir=LOG_DIR)\n",
        "    print('TensorboardX summary writer created')\n",
        "\n",
        "    # create model\n",
        "    alexnet = AlexNet(num_classes=NUM_CLASSES).to(device)\n",
        "    print('AlexNet created')\n",
        "\n",
        "    # create dataset and data loader\n",
        "    dataset = datasets.ImageFolder(TRAIN_IMG_DIR, transforms.Compose([\n",
        "        # transforms.RandomResizedCrop(IMAGE_DIM, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "        transforms.CenterCrop(IMAGE_DIM),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]))\n",
        "    print('Dataset created')\n",
        "    dataloader = data.DataLoader(\n",
        "        dataset,\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "        drop_last=True,\n",
        "        batch_size=BATCH_SIZE)\n",
        "    print('Dataloader created')\n",
        "\n",
        "    # create optimizer\n",
        "    # the one that WORKS\n",
        "    optimizer = optim.Adam(params=alexnet.parameters(), lr=0.0001)\n",
        "    ### BELOW is the setting proposed by the original paper - which doesn't train....\n",
        "    # optimizer = optim.SGD(\n",
        "    #     params=alexnet.parameters(),\n",
        "    #     lr=LR_INIT,\n",
        "    #     momentum=MOMENTUM,\n",
        "    #     weight_decay=LR_DECAY)\n",
        "    print('Optimizer created')\n",
        "\n",
        "    # multiply LR by 1 / 10 after every 30 epochs\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "    print('LR Scheduler created')\n",
        "\n",
        "    # start training!!\n",
        "    print('Starting training...')\n",
        "    total_steps = 1\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        lr_scheduler.step()\n",
        "        Acc_temp = 0\n",
        "        for imgs, classes in dataloader:\n",
        "            imgs, classes = imgs.to(device), classes.to(device)\n",
        "\n",
        "            # calculate the loss\n",
        "            output = alexnet(imgs)\n",
        "            loss = F.cross_entropy(output, classes)\n",
        "\n",
        "            # update the parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # log the information and add to tensorboard\n",
        "            if total_steps % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    _, preds = torch.max(output, 1)\n",
        "                    accuracy = torch.sum(preds == classes)\n",
        "\n",
        "                    print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n",
        "                        .format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n",
        "                    Acc_temp += accuracy.item()\n",
        "                    tbwriter.add_scalar('loss', loss.item(), total_steps)\n",
        "                    tbwriter.add_scalar('accuracy', accuracy.item(), total_steps)\n",
        "\n",
        "            # print out gradient values and parameter average values\n",
        "            if total_steps % 100 == 0:\n",
        "                with torch.no_grad():\n",
        "                    # print and save the grad of the parameters\n",
        "                    # also print and save parameter values\n",
        "                    print('*' * 10)\n",
        "                    for name, parameter in alexnet.named_parameters():\n",
        "                        if parameter.grad is not None:\n",
        "                            avg_grad = torch.mean(parameter.grad)\n",
        "                            #print('\\t{} - grad_avg: {}'.format(name, avg_grad))\n",
        "                            tbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_steps)\n",
        "                            tbwriter.add_histogram('grad/{}'.format(name),\n",
        "                                    parameter.grad.cpu().numpy(), total_steps)\n",
        "                        if parameter.data is not None:\n",
        "                            avg_weight = torch.mean(parameter.data)\n",
        "                            #print('\\t{} - param_avg: {}'.format(name, avg_weight))\n",
        "                            tbwriter.add_histogram('weight/{}'.format(name),\n",
        "                                    parameter.data.cpu().numpy(), total_steps)\n",
        "                            tbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_steps)\n",
        "\n",
        "            total_steps += 1\n",
        "        print('Accuracy:', Acc_temp,'/1045' )\n",
        "\n",
        "        # save checkpoints\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, 'alexnet_states_e{}.pkl'.format(epoch + 1))\n",
        "        state = {\n",
        "            'epoch': epoch,\n",
        "            'total_steps': total_steps,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model': alexnet.state_dict(),\n",
        "            'seed': seed,\n",
        "        }\n",
        "        torch.save(state, checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Used seed : 16904014131702502576\n",
            "TensorboardX summary writer created\n",
            "AlexNet created\n",
            "Dataset created\n",
            "Dataloader created\n",
            "Optimizer created\n",
            "LR Scheduler created\n",
            "Starting training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tStep: 10 \tLoss: 0.0000 \tAcc: 4\n",
            "Epoch: 1 \tStep: 20 \tLoss: 0.0000 \tAcc: 4\n",
            "Epoch: 1 \tStep: 30 \tLoss: 135.4017 \tAcc: 0\n",
            "Epoch: 1 \tStep: 40 \tLoss: 0.1569 \tAcc: 4\n",
            "Epoch: 1 \tStep: 50 \tLoss: 0.0003 \tAcc: 4\n",
            "Epoch: 1 \tStep: 60 \tLoss: 3.4781 \tAcc: 0\n",
            "Epoch: 1 \tStep: 70 \tLoss: 3.5063 \tAcc: 0\n",
            "Epoch: 1 \tStep: 80 \tLoss: 2.3155 \tAcc: 4\n",
            "Epoch: 1 \tStep: 90 \tLoss: 3.1518 \tAcc: 0\n",
            "Epoch: 1 \tStep: 100 \tLoss: 1.3614 \tAcc: 4\n",
            "**********\n",
            "Epoch: 1 \tStep: 110 \tLoss: 0.0074 \tAcc: 4\n",
            "Epoch: 1 \tStep: 120 \tLoss: 3.4430 \tAcc: 0\n",
            "Epoch: 1 \tStep: 130 \tLoss: 2.3126 \tAcc: 4\n",
            "Epoch: 1 \tStep: 140 \tLoss: 10.3736 \tAcc: 0\n",
            "Epoch: 1 \tStep: 150 \tLoss: 2.9925 \tAcc: 0\n",
            "Epoch: 1 \tStep: 160 \tLoss: 2.0725 \tAcc: 3\n",
            "Epoch: 1 \tStep: 170 \tLoss: 4.8652 \tAcc: 0\n",
            "Epoch: 1 \tStep: 180 \tLoss: 3.1562 \tAcc: 0\n",
            "Epoch: 1 \tStep: 190 \tLoss: 2.5877 \tAcc: 0\n",
            "Epoch: 1 \tStep: 200 \tLoss: 3.8907 \tAcc: 0\n",
            "**********\n",
            "Epoch: 1 \tStep: 210 \tLoss: 3.0984 \tAcc: 0\n",
            "Epoch: 1 \tStep: 220 \tLoss: 1.8105 \tAcc: 0\n",
            "Epoch: 1 \tStep: 230 \tLoss: 3.5760 \tAcc: 0\n",
            "Epoch: 1 \tStep: 240 \tLoss: 3.0382 \tAcc: 0\n",
            "Epoch: 1 \tStep: 250 \tLoss: 5.2822 \tAcc: 0\n",
            "Epoch: 1 \tStep: 260 \tLoss: 3.5865 \tAcc: 0\n",
            "Epoch: 1 \tStep: 270 \tLoss: 3.2789 \tAcc: 0\n",
            "Epoch: 1 \tStep: 280 \tLoss: 4.1001 \tAcc: 0\n",
            "Epoch: 1 \tStep: 290 \tLoss: 3.5165 \tAcc: 0\n",
            "Epoch: 1 \tStep: 300 \tLoss: 3.1476 \tAcc: 0\n",
            "**********\n",
            "Epoch: 1 \tStep: 310 \tLoss: 3.8131 \tAcc: 0\n",
            "Epoch: 1 \tStep: 320 \tLoss: 3.3901 \tAcc: 0\n",
            "Epoch: 1 \tStep: 330 \tLoss: 2.7348 \tAcc: 0\n",
            "Epoch: 1 \tStep: 340 \tLoss: 3.7447 \tAcc: 0\n",
            "Epoch: 1 \tStep: 350 \tLoss: 3.4248 \tAcc: 0\n",
            "Epoch: 1 \tStep: 360 \tLoss: 4.0792 \tAcc: 0\n",
            "Epoch: 1 \tStep: 370 \tLoss: 3.5238 \tAcc: 0\n",
            "Epoch: 1 \tStep: 380 \tLoss: 3.2148 \tAcc: 0\n",
            "Epoch: 1 \tStep: 390 \tLoss: 4.4998 \tAcc: 0\n",
            "Epoch: 1 \tStep: 400 \tLoss: 3.5802 \tAcc: 0\n",
            "**********\n",
            "Epoch: 1 \tStep: 410 \tLoss: 3.1750 \tAcc: 0\n",
            "Epoch: 1 \tStep: 420 \tLoss: 3.8729 \tAcc: 0\n",
            "Epoch: 1 \tStep: 430 \tLoss: 3.3364 \tAcc: 0\n",
            "Epoch: 1 \tStep: 440 \tLoss: 2.6920 \tAcc: 0\n",
            "Epoch: 1 \tStep: 450 \tLoss: 3.7819 \tAcc: 0\n",
            "Epoch: 1 \tStep: 460 \tLoss: 3.4539 \tAcc: 0\n",
            "Epoch: 1 \tStep: 470 \tLoss: 4.2420 \tAcc: 0\n",
            "Epoch: 1 \tStep: 480 \tLoss: 3.5744 \tAcc: 0\n",
            "Epoch: 1 \tStep: 490 \tLoss: 3.0043 \tAcc: 0\n",
            "Epoch: 1 \tStep: 500 \tLoss: 4.6303 \tAcc: 0\n",
            "**********\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}